{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DLFqCYQVFcC"
   },
   "source": [
    "# LLM-supported workflow for processing faulty OCR\n",
    "\n",
    "*Notebook based on Sarah Oberbichler's (oberbichler@ieg-mainz.de) Notebook "Researching German Historical Newspapers with Llama AI Model" (https://github.com/soberbichler/Notebooks4Historical_Newspapers/blob/main/Llama3_OCR.ipynb)*<br>\n",
    "*Edited by Christian Wachter [![ORCID](https://info.orcid.org/wp-content/uploads/2019/11/orcid_16x16.png)](https://orcid.org/0000-0003-2937-0868) (christian.wachter@uni-bielefeld.de) and Patrick Jentsch (p.jentsch@uni-bielefeld.de)*\n",
    "\n",
    "This notebook shows how LLMs can be used to support research with historical newspapers. In this example, the Llama 3.1 model is used to correct OCR of previously OCR'd historical newspaper pages.\n",
    "\n",
    "OCR quality has been a long-standing issue in digitization efforts. Historical newspapers are particularly affected due their complexity, historical fonts, or degradation. Additionally, OCR technology faced limitations when dealing with historical scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German Historical Newspaper Portal\n",
    "\n",
    "German historical newspapers from the German Digital Library can be accessed via the DDB-API. This API is open access and allows to query the Historical Newspapers available in the German Newspaper Portal ([Deutsches Zeitungsportal](https://www.deutsche-digitale-bibliothek.de/newspaper)). An instruction, provided by the German Newspaper Portal (from Karl Krägerlin), can be found [here](https://deepnote.com/app/karl-kragelin-b83c/Zeitungsportal-API-d9224dda-8e26-4b35-a6d7-40e9507b1151)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17844,
     "status": "ok",
     "timestamp": 1730817169999,
     "user": {
      "displayName": "C W",
      "userId": "01688883033358762912"
     },
     "user_tz": -60
    },
    "id": "f9ysGwj_KLa-",
    "outputId": "d3573726-740e-4b95-c0ae-704501fcd078"
   },
   "outputs": [],
   "source": [
    "# Install the \"Deutsche Digitale Bibliothek API\" package (ddbapi: https://pypi.org/project/ddbapi/)\n",
    "%pip install ddbapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the newspapers to be processed for case studies\n",
    "\n",
    "Every newspaper accessible through the German Digital Library has an ID provided by the Zeitschriftendatenbank (ZDB). With these IDs, we can target specific newspapers. For case studies, a relevant selection of ZDB-IDs can be saved in lists. This allows case-study-specific selections to be easily utilized in subsequent steps of the workflow. Additionally, time periods relevant to the case studies can be defined and saved in variables, enabling further refinement of the scope in subsequent steps of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzW_y_D7ffhD"
   },
   "outputs": [],
   "source": [
    "zdb_ids = [\n",
    "    # Vorwärts : Berliner Volksblatt ; das Abendblatt der Hauptstadt Deutschlands\n",
    "    \"2814128-3\"\n",
    "]\n",
    "\n",
    "periods = [\n",
    "    \"[1921-08-10T00:00:01Z TO 1921-08-12T23:59:59Z]\",\n",
    "    \"[1922-08-10T00:00:01Z TO 1922-08-12T23:59:59Z]\",\n",
    "    \"[1923-08-10T00:00:01Z TO 1923-08-12T23:59:59Z]\",\n",
    "    \"[1924-08-10T00:00:01Z TO 1924-08-12T23:59:59Z]\",\n",
    "    \"[1925-08-10T00:00:01Z TO 1925-08-12T23:59:59Z]\",\n",
    "    \"[1926-08-10T00:00:01Z TO 1926-08-12T23:59:59Z]\",\n",
    "    \"[1927-08-10T00:00:01Z TO 1927-08-12T23:59:59Z]\",\n",
    "    \"[1928-08-10T00:00:01Z TO 1928-08-12T23:59:59Z]\",\n",
    "    \"[1929-08-10T00:00:01Z TO 1929-08-12T23:59:59Z]\",\n",
    "    \"[1930-08-10T00:00:01Z TO 1930-08-12T23:59:59Z]\",\n",
    "    \"[1931-08-10T00:00:01Z TO 1931-08-12T23:59:59Z]\",\n",
    "    \"[1932-08-10T00:00:01Z TO 1932-08-12T23:59:59Z]\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for DDB API interaction\n",
    "\n",
    "Down below are two helper functions for querying the DDB API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ddbapi\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_issues(publication_date: str, zdb_ids: list[str]) -> pd.DataFrame:\n",
    "    \"\"\" Get data on Newspaper-Issues from the DDB API. \"\"\"\n",
    "    data_frames = [\n",
    "        ddbapi.zp_issues(publication_date=publication_date, zdb_id=zdb_id)\n",
    "        for zdb_id in zdb_ids\n",
    "    ]\n",
    "    return pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "\n",
    "def get_pages(publication_dates: list[str], zdb_ids: list[str], keywords: list[str]) -> pd.DataFrame:\n",
    "    \"\"\" Get data on Newspaper-Pages from the DDB API for multiple periods and keywords. \"\"\"\n",
    "    data_frames = []\n",
    "\n",
    "    for publication_date in publication_dates:\n",
    "        for zdb_id in zdb_ids:\n",
    "            for keyword in keywords:\n",
    "                try:\n",
    "                    df = ddbapi.zp_pages(publication_date=publication_date, zdb_id=zdb_id, plainpagefulltext=keyword)\n",
    "                    if isinstance(df, pd.DataFrame):\n",
    "                        data_frames.append(df)\n",
    "                    else:\n",
    "                        print(f\"Warning: The result for date {publication_date}, zdb_id {zdb_id}, and keyword {keyword} is not a DataFrame.\")\n",
    "                except requests.exceptions.HTTPError as e:\n",
    "                    print(f\"HTTPError for date {publication_date}, zdb_id {zdb_id}, and keyword {keyword}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred for date {publication_date}, zdb_id {zdb_id}, and keyword {keyword}: {e}\")\n",
    "\n",
    "    if len(data_frames) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    return combined_df.drop_duplicates(subset=['page_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define search terms and/or phrases to effectively query the German Historical Newspaper Portal\n",
    "\n",
    "Searching for specific terms or phrases retrieves only newspaper pages that include those keywords or phrases. We can define them individually or create thematic lists to use in the queries. Below, you find a selection of basic keyword lists. They may be combined to create specified keyword lists for more targeted queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of keywords centered on discussions about democracy in general\n",
    "keywords_democracy = [\n",
    "    \"demokrat*\",\n",
    "    \"volksherrschaft\",\n",
    "    \"demokratisier*\",\n",
    "    \"verfassung*\"\n",
    "]\n",
    "\n",
    "# A list of keywords specified on social democratic topoi in discourses on democracy\n",
    "keywords_social_democrats = [\n",
    "    \"sozialisier*\",\n",
    "    \"sozialistisch~\",\n",
    "    \"verfassung*\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31920,
     "status": "ok",
     "timestamp": 1730817477469,
     "user": {
      "displayName": "C W",
      "userId": "01688883033358762912"
     },
     "user_tz": -60
    },
    "id": "5FLflpskwDq5",
    "outputId": "f9c3a18b-6f34-4768-c7a1-4cc5a1935faf"
   },
   "outputs": [],
   "source": [
    "def combine_keyword_lists(*keyword_lists: list[str]) -> list[str]:\n",
    "    \"\"\" Combines multiple lists of keywords into and removes duplicates. \"\"\"\n",
    "    # Use a set to combine all keywords and automatically remove duplicates\n",
    "    combined_set = set()\n",
    "    for keyword_list in keyword_lists:\n",
    "        combined_set.update(keyword_list)\n",
    "    # Convert the set back to a list\n",
    "    keywords_combined = list(combined_set)\n",
    "    return keywords_combined\n",
    "\n",
    "# Create a specified list of keywords by combining basic keyword lists\n",
    "keywords = combine_keyword_lists(\n",
    "    keywords_democracy,\n",
    "    keywords_social_democrats\n",
    ")\n",
    "\n",
    "# Get data on newspapers to query\n",
    "pages = get_pages(\n",
    "    periods,\n",
    "    zdb_ids,\n",
    "    keywords\n",
    ")\n",
    "\n",
    "print(f\"Number of entries: {len(pages)}\")\n",
    "pages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrowing down the text around a keyword (= extracting a context window)\n",
    "By selecting only a context window of text around a keyword we reduce the input tokens that are later processed by the LLM for OCR-postcorrection. To do so, we define the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_context(keywords: list[str], text: str, tokens_before: int, tokens_after: int) -> str:\n",
    "    # Lowercase the text for case-insensitive matching\n",
    "    text = text.lower()\n",
    "\n",
    "    # Find all keyword positions in the text using the Python library regex\n",
    "    keyword_positions = []\n",
    "\n",
    "    for keyword in keywords:\n",
    "        # Replace wildcard with regex equivalent\n",
    "        keyword = keyword.replace('*', '\\\\w*')\n",
    "        pattern = re.compile(keyword)\n",
    "\n",
    "        for match in pattern.finditer(text):\n",
    "            keyword_positions.append(match.start())\n",
    "\n",
    "    if not keyword_positions:\n",
    "        return \"NN\"\n",
    "\n",
    "    # Defining the start and ending positions of the context window\n",
    "    first_occurrence = min(keyword_positions)\n",
    "    last_occurrence = max(keyword_positions)\n",
    "\n",
    "    # Calculating the start of the context window\n",
    "    start_index = max(0, first_occurrence - tokens_before)\n",
    "\n",
    "    # Calculating the end of the context window\n",
    "    end_index = min(len(text), last_occurrence + tokens_after)\n",
    "\n",
    "    # Extracting the context window\n",
    "    context = text[start_index:end_index]\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the context window and attach it to the data frame\n",
    "pages['context'] = [\n",
    "    extract_context(keywords, row['plainpagefulltext'], 200, 200)\n",
    "    for _, row in pages.iterrows()\n",
    "]\n",
    "\n",
    "pages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing keyword findings through frequency charts\n",
    "\n",
    "First, we extract information on the count of keyword search hits for each day from the above data frame. We store that information in a new data frame. Then we define a visualization function for simple frequency analysis of keyword search hits over time. This function can be used flexibly for different keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust: As it is, all the df entries are counted (and later visualized), not just the ones containing the search terms!!!!!!!!!!!!!!!!\n",
    "pages['publication_date'] = pd.to_datetime(pages['publication_date'])\n",
    "pages['year'] = pages['publication_date'].dt.year\n",
    "pages['month'] = pages['publication_date'].dt.month\n",
    "pages['day'] = pages['publication_date'].dt.day\n",
    "daily_counts = pages.groupby(['year', 'month', 'day']).size().reset_index(name='count')\n",
    "print(daily_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def linegraph_search_hits(df: pd.DataFrame, keywords: list[str], time_granularity: str, xlabel: str, ylabel: str = 'Count'):\n",
    "    '''Plots keyword search hits over time with specified granularity (pass 'day', 'month', or 'year').'''\n",
    "    sns.set_theme(style=\"darkgrid\", font_scale=1.0)\n",
    "\n",
    "    # Create a figure for plotting\n",
    "    plt.figure(figsize=(18, 12))\n",
    "\n",
    "    # Create a DataFrame to hold all the data for plotting\n",
    "    plot_data = pd.DataFrame()\n",
    "\n",
    "    for keyword in keywords:\n",
    "        # Filter DataFrame for rows containing the keyword in the 'context' column\n",
    "        keyword_df = df[df['context'].str.contains(keyword, case=False, na=False)].copy()\n",
    "\n",
    "        # Group by the specified time granularity\n",
    "        if time_granularity == 'year':\n",
    "            grouped_df = keyword_df.groupby('year').size().reset_index(name='count')\n",
    "            grouped_df['time'] = pd.to_datetime(grouped_df['year'].astype(str))\n",
    "        elif time_granularity == 'month':\n",
    "            grouped_df = keyword_df.groupby(['year', 'month']).size().reset_index(name='count')\n",
    "            grouped_df['time'] = pd.to_datetime(grouped_df['year'].astype(str) + '-' + grouped_df['month'].astype(str).str.zfill(2))\n",
    "        elif time_granularity == 'day':\n",
    "            grouped_df = keyword_df.groupby(['year', 'month', 'day']).size().reset_index(name='count')\n",
    "            grouped_df['time'] = pd.to_datetime(grouped_df[['year', 'month', 'day']])\n",
    "        else:\n",
    "            raise ValueError(\"time_granularity must be 'year', 'month', or 'day'\")\n",
    "\n",
    "        # Add a column for the keyword to use in hue and style\n",
    "        grouped_df['keyword'] = keyword\n",
    "\n",
    "        # Append to the plot data\n",
    "        plot_data = pd.concat([plot_data, grouped_df], ignore_index=True)\n",
    "\n",
    "    # Check if plot_data is empty\n",
    "    if plot_data.empty:\n",
    "        print(\"No data available for the given keywords.\")\n",
    "        return\n",
    "\n",
    "    # Sort the intermediary plot_data DataFrame by the 'keyword' first and then 'time' column\n",
    "    plot_data = plot_data.sort_values(by=['keyword', 'time'])\n",
    "\n",
    "    # Format 'time' as year-month for plotting when time_granularity is 'month'\n",
    "    if time_granularity == 'year':\n",
    "        plot_data['time'] = plot_data['time'].dt.strftime('%Y')\n",
    "    elif time_granularity == 'month':\n",
    "        plot_data['time'] = plot_data['time'].dt.strftime('%Y-%m')\n",
    "    elif time_granularity == 'day':\n",
    "        plot_data['time'] = plot_data['time'].dt.strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid time granularity specified. Choose from 'year', 'month', or 'day'.\")\n",
    "\n",
    "    # Plot using hue and style for different keywords\n",
    "    sns.lineplot(data=plot_data, x='time', y='count', hue='keyword', style='keyword', markers=True, markersize=8, dashes=False)\n",
    "\n",
    "    # Customize the plot's appearence\n",
    "    plt.xlabel(xlabel, fontsize=13)\n",
    "    plt.ylabel(ylabel, fontsize=13)\n",
    "    plt.title('Search hits for keywords', fontsize=16)\n",
    "    plt.xticks(rotation=65)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=11)\n",
    "    plt.legend(loc='upper left', fontsize=12, title='Keywords', title_fontsize='14', framealpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your time granularity\n",
    "time_granularity = 'month'\n",
    "\n",
    "# Call the function, using the same variable for both `time_granularity` and `xlabel`\n",
    "linegraph_search_hits(\n",
    "    pages, # pass the text data (from the 'context' column of a data frame)\n",
    "    keywords, # Pass the keyword(s) you want to use\n",
    "    time_granularity=time_granularity,  # Pass the time granularity\n",
    "    xlabel=time_granularity.capitalize()  # Use the same variable for xlabel, with capitalization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KLdlJqpYxrd"
   },
   "source": [
    "## Get started with Ollama LLMs using Python\n",
    "\n",
    "Ollama LLMs are large language models designed to perform various natural language processing tasks. These models require significant computational resources and are typically executed on a dedicated server. To interact with these models, we need a client that connects to the server where the models are hosted.\n",
    "\n",
    "In our case, we set up the client using a `.env` file that contains the necessary configuration details, such as the server host and authorization headers. This setup ensures secure and efficient communication between the client and the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "First, we need to install the `ollama` package to interact with the Ollama LLMs and the `python-dotenv` package to manage environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ollama python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Environment Variables\n",
    "\n",
    "We use the `dotenv` package to load the environment variables from a `.env` file. This file should contain the server host and authorization headers required to connect to the Ollama server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "\n",
    "env = dotenv.dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up the Ollama Client\n",
    "\n",
    "With the environment variables loaded, we can set up the Ollama client. This client will be used to send requests to the server and receive responses from the LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "ollama_client = ollama.Client(\n",
    "    host=env[\"OLLAMA_HOST\"],\n",
    "    headers={\n",
    "        \"Authorization\": env[\"OLLAMA_AUTHORIZATION_HEADER\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeZ7GcEkQ3n8"
   },
   "source": [
    "### Run model for OCR-post correction\n",
    "\n",
    "To run OCR-post correction, it is essential to formulate a precise prompt. For example, it needs to be specified that the whole text should be corrected, while summarizations and any other addition need to be avoided. A guide on how to write effective prompts can be found also [here](https://https://support.google.com/a/users/answer/14200040?hl=en).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Model Options\n",
    "\n",
    "Before running the model, we need to configure various options such as the context window size, temperature, and other parameters that control the model's behavior. A documentation about available options can found here: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    # Sets the size of the context window used to generate the next token. (Default: 2048)\n",
    "    \"num_ctx\": 4096,\n",
    "    # The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)\n",
    "    \"temperature\": 0.75,\n",
    "    # Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\n",
    "    \"top_p\": 0.35,\n",
    "    # Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0)\n",
    "    \"mirostat_tau\": 2.0,\n",
    "    # Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate stop parameters in a modelfile.\n",
    "    #\"stop\": \"NN\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for Ollama interaction\n",
    "\n",
    "Down below are two helper functions for LLM interactions. Each utilizes a different technical approach to compare the quality of each approach or spot differences.\n",
    "\n",
    "1. **Chat-based Interaction**: This function uses the `/chat` endpoint to perform OCR post-correction by sending a series of messages to the model.\n",
    "2. **Generate-based Interaction**: This function uses the `/generate` endpoint to perform OCR post-correction by sending a single prompt to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chat based interaction\n",
    "\n",
    "Create a chat with an existing conversation, implementing some Prompt Engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    # Role Prompting + Instructions + Encouraging (by remembering the importance of good results) + Chain-of-Thought\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\":\n",
    "\"\"\"\n",
    "You are a precise and methodical assistant tasked with postprocessing faulty Optical Character Recognition (OCR) results. Your primary responsibility is to identify and correct only those errors in the text that are the result of faulty OCR transcription. Follow these instructions carefully and step by step:\n",
    "\n",
    "1. **Input Context**:\n",
    "- The text originates from historical German newspapers from the Weimar Republic era, originally written in blackletter (Fraktur).\n",
    "- OCR transcription is inconsistent: some sections are transcribed well, others poorly. Focus only on correcting obvious OCR errors.\n",
    "\n",
    "2. **Special Characters and Normalizations**:\n",
    "- Handle German-specific characters correctly, including:\n",
    "    - `ö`/`Ö`, `ä`/`Ä`, `ü`/`Ü`, and `ß`.\n",
    "- Normalize the historical character `ſ` by converting it to `s` in your output.\n",
    "- Correct incorrectly hyphenated words. If a word is split by a hyphen but does not logically belong separated, reconstruct the word as it should appear correctly.\n",
    "- Preserve all other historical spellings exactly as written in the input text. Do not modernize spellings or apply contemporary grammatical rules.\n",
    "\n",
    "3. **Scope of Corrections**:\n",
    "- Only correct words that are clearly misspelled due to OCR errors.\n",
    "- Do not correct or alter other types of errors, such as historical grammatical variations or outdated spellings, unless they result directly from OCR faults.\n",
    "- Maintain the structure of the text, ensuring no additions, omissions, or rephrasing beyond the corrections.\n",
    "\n",
    "4. **Behavioral Guidelines**:\n",
    "- Do not ask for feedback, clarifications, or pose any questions.\n",
    "- Do not provide comments, summaries, or explanations. Simply provide your OCR-corrected output text.\n",
    "- If the input text consists solely of \"NN\", simply return \"NN\".\n",
    "\n",
    "**Your Output**:\n",
    "- The corrected text, with only the faulty OCR transcriptions addressed and corrected, as described above.\n",
    "\n",
    "It is essential to adhere strictly to these guidelines to ensure accurate and reliable results. Please do a good job, since this is essential for my work.\n",
    "\"\"\".strip()\n",
    "    },\n",
    "    # Few-Shot Prompting\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Wir kümpfen für iene Sfunde.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Wir kämpfen für jene Stunde.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Die Rüftung der Truppen iſt vorangegangen.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Die Rüstung der Truppen ist vorangegangen.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Die V lksgemeinfchaft war zentral or- ganisiert.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Die Volksgemeinschaft war zentral organisiert.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_ocr_post_correction_chat(text: str, model: str = \"llama3.1:8b\") -> str:\n",
    "    ''' Perform OCR Post Correction with ollama using the /chat endpoint. '''\n",
    "    response_buffer = []\n",
    "\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    is_first_paragraph = True\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph.strip() == '':\n",
    "            continue\n",
    "\n",
    "        if is_first_paragraph:\n",
    "            is_first_paragraph = False\n",
    "        else:\n",
    "            response_buffer.append(\"\\n\\n\")\n",
    "\n",
    "        message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text\n",
    "        }\n",
    "\n",
    "        for chunked_chat_response in ollama_client.chat(\n",
    "            model=model,\n",
    "            messages=[*messages, message],\n",
    "            options=options,\n",
    "            stream=True\n",
    "        ):\n",
    "            response_buffer.append(chunked_chat_response[\"message\"][\"content\"])\n",
    "\n",
    "    return ''.join(response_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate based interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \\\n",
    "\"\"\"\n",
    "You are a precise and methodical assistant tasked with postprocessing faulty Optical Character Recognition (OCR) results. Your primary responsibility is to identify and correct only those errors in the text that are the result of faulty OCR transcription. Follow these instructions carefully and step by step:\n",
    "\n",
    "1. **Input Context**:\n",
    "- The text originates from historical German newspapers from the Weimar Republic era, originally written in blackletter (Fraktur).\n",
    "- OCR transcription is inconsistent: some sections are transcribed well, others poorly. Focus only on correcting obvious OCR errors.\n",
    "\n",
    "2. **Special Characters and Normalizations**:\n",
    "- Handle German-specific characters correctly, including:\n",
    "    - `ö`/`Ö`, `ä`/`Ä`, `ü`/`Ü`, and `ß`.\n",
    "- Normalize the historical character `ſ` by converting it to `s` in your output.\n",
    "- Correct incorrectly hyphenated words. If a word is split by a hyphen but does not logically belong separated, reconstruct the word as it should appear correctly.\n",
    "- Preserve all other historical spellings exactly as written in the input text. Do not modernize spellings or apply contemporary grammatical rules.\n",
    "\n",
    "3. **Scope of Corrections**:\n",
    "- Only correct words that are clearly misspelled due to OCR errors.\n",
    "- Do not correct or alter other types of errors, such as historical grammatical variations or outdated spellings, unless they result directly from OCR faults.\n",
    "- Maintain the structure of the text, ensuring no additions, omissions, or rephrasing beyond the corrections.\n",
    "\n",
    "4. **Behavioral Guidelines**:\n",
    "- Do not ask for feedback, clarifications, or pose any questions.\n",
    "- Do not provide comments, summaries, or explanations. Simply provide your OCR-corrected output text.\n",
    "- If the input text consists solely of \"NN\", simply return \"NN\".\n",
    "\n",
    "**Your Output**:\n",
    "- The corrected text, with only the faulty OCR transcriptions addressed and corrected, as described above.\n",
    "\n",
    "**Examples**:\n",
    "Input: Wir kümpfen für iene Sfunde.\n",
    "Output: Wir kämpfen für jene Stunde.\n",
    "\n",
    "Input: Die Rüftung der Truppen iſt vorangegangen.\n",
    "Output: Die Rüstung der Truppen ist vorangegangen.\n",
    "\n",
    "Input: Die V lksgemeinfchaft war zentral or- ganisiert.\n",
    "Output: Die Volksgemeinschaft war zentral organisiert.\n",
    "\n",
    "It is essential to adhere strictly to these guidelines to ensure accurate and reliable results. Please do a good job, since this is essential for my work.\n",
    "\"\"\".strip()\n",
    "\n",
    "template = \\\n",
    "\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{ .System }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ .Prompt }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "Here is the corrected text:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_ocr_post_correction_generate(text: str, model: str = \"llama3.1:8b\") -> str:\n",
    "    ''' Perform OCR Post Correction with ollama using the /generate endpoint. '''\n",
    "    response_buffer = []\n",
    "\n",
    "    for chunked_generate_response in ollama_client.generate(\n",
    "        model=model,\n",
    "        system=system,\n",
    "        prompt=text,\n",
    "        template=template,\n",
    "        options=options,\n",
    "        stream=True\n",
    "    ):\n",
    "        response_buffer.append(chunked_generate_response['response'])\n",
    "\n",
    "    return ''.join(response_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform OCR Post Correction\n",
    "\n",
    "To perform OCR post-correction, we will use the Ollama LLMs. We have defined two helper functions: one for chat-based interaction and another for generate-based interaction. We will iterate over the collected pages, apply both methods, and store the corrected text in a new column of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on the size of the dataframe, it can take a while to load.\n",
    "print(\"Amount of texts to be processed:\", (len(pages)))\n",
    "\n",
    "# Create an empty list to store the corrected pages\n",
    "results = []\n",
    "\n",
    "# Iterate over our collected pages\n",
    "for index, row in pages.iterrows():\n",
    "    text = row[\"context\"]\n",
    "\n",
    "    print(f\"## Input {index}\\n{text}\")\n",
    "\n",
    "    # perform ollama ocr post correction\n",
    "    result_1 = ollama_ocr_post_correction_chat(text, model=\"gemma3:12b\")\n",
    "    # result_2 = ollama_ocr_post_correction_generate(text, model=\"granite3.3:8b\")\n",
    "\n",
    "    print(f\"## Output {index}.chat\\n{result_1}\")\n",
    "    # print(f\"## Output {index}.generate\\n{result_2}\")\n",
    "    print(\"\")\n",
    "\n",
    "    results.append(result_1)\n",
    "    #old: results.append((result_1, result_2))\n",
    "\n",
    "# Adding a new column with the corrected text to the DataFrame while ensuring the results list has the same length as the DataFrame\n",
    "if len(results) == len(pages):\n",
    "    pages.loc[:, 'context_postcorrected'] = results\n",
    "else:\n",
    "    print(\"Warning: The length of results does not match the number of rows in the DataFrame.\")\n",
    "\n",
    "pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the data frame to a csv file\n",
    "pages.to_csv(\"results.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1sN3MQooM6aOEM_2KM0yNfALYoLYJi9DT",
     "timestamp": 1732190952815
    },
    {
     "file_id": "1Wg2JjNZfl1CKgdy-BXdPJPYGWYOicDNE",
     "timestamp": 1721031400834
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
